{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import BinaryLovaszHingeLoss, DiceLoss, JaccardLoss, dice_coefficient, show_images_and_masks\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fix seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augment=False, denoise=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.denoise = denoise\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "        self.augment = augment\n",
    "\n",
    "        self.augment_transform = A.Compose([\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=180, p=0.5),\n",
    "                A.ElasticTransform(alpha=10, sigma=50, alpha_affine=80, p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.2, rotate_limit=45, p=0.30),\n",
    "            ], p=1.0),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.GaussianBlur(blur_limit=3, p=0.5),\n",
    "            A.GaussNoise(var_limit=(10, 50), p=0.20),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomResizedCrop(height=512, width=512, scale=(0.8, 1.0), ratio=(0.75, 1.33), p=0.15),\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(30,30), p=1),\n",
    "            #A.CoarseDropout(max_holes=16, max_height=50, max_width=40, min_holes=8, min_height=20, min_width=20, fill_value=0, p=0.5),\n",
    "            A.ToFloat(max_value=255.0),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.basic_transform = A.Compose([\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(30,30), p=1),\n",
    "            A.ToFloat(max_value=255.0),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        mask_path = os.path.join(self.mask_dir, image_filename.replace('image', 'mask'))\n",
    "\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.denoise:\n",
    "            image = cv2.fastNlMeansDenoising(image, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "        image_pil = Image.fromarray(image).convert('RGB')  \n",
    "        mask_pil = Image.fromarray(mask)\n",
    "\n",
    "        image_pil = image_pil.resize((512, 512), Image.LANCZOS)\n",
    "        mask_pil = mask_pil.resize((512, 512), Image.LANCZOS)\n",
    "\n",
    "        image = np.array(image_pil)\n",
    "        mask = np.array(mask_pil)\n",
    "        _, mask = cv2.threshold(mask, 127, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "        if self.augment:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "        else:\n",
    "            augmented = self.basic_transform(image=image, mask=mask)\n",
    "\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "\n",
    "        return image, mask.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all_encoder_layers(model):\n",
    "    for layer in model.down_blocks:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_block(model, block_name):\n",
    "    \"\"\"\n",
    "    Unfreezes a specific block of the ResNet encoder within the U-Net model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The instance of your UNetWithResnet50Encoder model.\n",
    "    - block_name: A string name of the block to unfreeze (e.g., 'layer4', 'layer3').\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'down_blocks'):\n",
    "        for block in model.down_blocks:\n",
    "            # The down_blocks attribute is a ModuleList; each block is a layer in ResNet\n",
    "            if block_name == block.__class__.__name__:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = True\n",
    "    else:\n",
    "        print(f\"The model does not have the specified block: {block_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(image, true_mask=None, predicted_mask=None):\n",
    "    \"\"\"Visualize comparison between input image, true mask, and predicted mask.\"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 10)) \n",
    "\n",
    "    axs[0].imshow(image, cmap='gray')\n",
    "    axs[0].set_title('Input Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    if true_mask is not None:\n",
    "        axs[1].imshow(true_mask, cmap='gray')\n",
    "        axs[1].set_title('True Mask')\n",
    "        axs[1].axis('off')\n",
    "    else:\n",
    "        axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(predicted_mask, cmap='gray')\n",
    "    axs[2].set_title('Predicted Mask')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, dataset, device, n_images):\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_images):\n",
    "            input_img, true_mask = dataset[i]\n",
    "            input_img_unsqueeze = input_img.unsqueeze(0).to(device)\n",
    "\n",
    "            # Predict\n",
    "            pred_mask_logits = model(input_img_unsqueeze)\n",
    "            pred_mask_prob = torch.sigmoid(pred_mask_logits)\n",
    "            pred_mask = pred_mask_prob > 0.5 \n",
    "\n",
    "            # Calculate Dice Coefficient\n",
    "            pred = pred_mask.squeeze()\n",
    "            dice_score = dice_coefficient(pred, true_mask.to(device), smooth=1e-6)\n",
    "\n",
    "            dice_scores.append(dice_score.item())\n",
    "\n",
    "            # Convert for visualization\n",
    "            pred_mask_np = pred_mask.squeeze().cpu().numpy() \n",
    "            input_img_np = input_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            input_img_np = (input_img_np * 255).astype(np.uint8)\n",
    "            true_mask_np = true_mask.squeeze().cpu().numpy() if true_mask is not None else None \n",
    "\n",
    "            visualize(\n",
    "                image=input_img_np, \n",
    "                true_mask=true_mask_np,\n",
    "                predicted_mask=pred_mask_np\n",
    "            )\n",
    "\n",
    "    # Print the average Dice score at the end \n",
    "    average_dice_score = np.mean(dice_scores) \n",
    "    print(f'Average Dice Coefficient over {n_images} images: {average_dice_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, jaccard_score\n",
    "\n",
    "\n",
    "def compute_specificity(y_true, y_pred):\n",
    "    tn, fp, _, _ = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    return specificity\n",
    "\n",
    "def evaluate_model(model, dataset, device):\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "\n",
    "    f1_scores = []\n",
    "    ious = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_img, true_mask in dataset:\n",
    "            input_img = input_img.unsqueeze(0).to(device)\n",
    "            true_mask = true_mask.unsqueeze(0).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_img)\n",
    "            pred_mask_prob = torch.sigmoid(output)\n",
    "            pred_mask = (pred_mask_prob > 0.5).float()\n",
    "\n",
    "            # Flatten the tensors for compatibility with sklearn metrics\n",
    "            y_true = true_mask.view(-1).cpu().numpy()\n",
    "            y_pred = pred_mask.view(-1).cpu().numpy()\n",
    "\n",
    "            # Compute metrics\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "            dice_score = dice_coefficient(pred_mask, true_mask, smooth=1e-6).item()\n",
    "            iou = jaccard_score(y_true, y_pred, zero_division=1)\n",
    "\n",
    "            # Append metrics\n",
    "            f1_scores.append(f1)\n",
    "            dice_scores.append(dice_score)\n",
    "            ious.append(iou)\n",
    "\n",
    "    # Compute mean of all metrics\n",
    "    metrics = {\n",
    "        'Mean F1 Score': np.mean(f1_scores),\n",
    "        'Mean Dice Coefficient': np.mean(dice_scores),\n",
    "        'Mean IoU': np.mean(ious)\n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alois\\AppData\\Roaming\\Python\\Python311\\site-packages\\albumentations\\augmentations\\blur\\transforms.py:189: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.9654\n",
      "Mean Dice Coefficient: 0.9654\n",
      "Mean IoU: 0.9332\n"
     ]
    }
   ],
   "source": [
    "from resnet50_unet import UNetWithResnet50Encoder\n",
    "\n",
    "name_model = 'best_model_retina_fine_tuned'\n",
    "\n",
    "test_dataset = CustomDataset('../data_retina/test/images', '../data_retina/test/masks', augment=False, denoise=False)\n",
    "\n",
    "model = UNetWithResnet50Encoder(n_classes=1).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'output_dir/{name_model}.pth'))\n",
    "model.eval()\n",
    "\n",
    "metrics = evaluate_model(model, test_dataset, device)\n",
    "\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f'{metric_name}: {metric_value:.4f}')\n",
    "\n",
    "#predict_and_visualize(model, test_dataset, device, n_images=len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predict_and_visualize(model, test_dataset, device, n_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_dataset))\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mpredict_and_visualize\u001b[1;34m(model, dataset, device, n_images)\u001b[0m\n\u001b[0;32m     23\u001b[0m         input_img_np \u001b[38;5;241m=\u001b[39m (input_img_np \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     24\u001b[0m         true_mask_np \u001b[38;5;241m=\u001b[39m true_mask\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m true_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[1;32m---> 26\u001b[0m         visualize(\n\u001b[0;32m     27\u001b[0m             image\u001b[38;5;241m=\u001b[39minput_img_np, \n\u001b[0;32m     28\u001b[0m             true_mask\u001b[38;5;241m=\u001b[39mtrue_mask_np,\n\u001b[0;32m     29\u001b[0m             predicted_mask\u001b[38;5;241m=\u001b[39mpred_mask_np\n\u001b[0;32m     30\u001b[0m         )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Print the average Dice score at the end \u001b[39;00m\n\u001b[0;32m     33\u001b[0m average_dice_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(dice_scores) \n",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m, in \u001b[0;36mvisualize\u001b[1;34m(image, true_mask, predicted_mask)\u001b[0m\n\u001b[0;32m     19\u001b[0m axs[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Mask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m axs[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:527\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    526\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         display(\n\u001b[0;32m     91\u001b[0m             figure_manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[0;32m     92\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m_fetch_figure_metadata(figure_manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure)\n\u001b[0;32m     93\u001b[0m         )\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m(obj, include\u001b[38;5;241m=\u001b[39minclude, exclude\u001b[38;5;241m=\u001b[39mexclude)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m formatter(obj)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m printer(obj)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\backend_bases.py:2161\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2161\u001b[0m         bbox_inches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mget_tightbbox(\n\u001b[0;32m   2162\u001b[0m             renderer, bbox_extra_artists\u001b[38;5;241m=\u001b[39mbbox_extra_artists)\n\u001b[0;32m   2163\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m                 pad_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2165\u001b[0m             h_pad \u001b[38;5;241m=\u001b[39m layout_engine\u001b[38;5;241m.\u001b[39mget()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh_pad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    449\u001b[0m     warn_deprecated(\n\u001b[0;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\figure.py:1783\u001b[0m, in \u001b[0;36mFigureBase.get_tightbbox\u001b[1;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax\u001b[38;5;241m.\u001b[39mget_visible():\n\u001b[0;32m   1780\u001b[0m     \u001b[38;5;66;03m# some axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1783\u001b[0m         bbox \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mget_tightbbox(\n\u001b[0;32m   1784\u001b[0m             renderer, bbox_extra_artists\u001b[38;5;241m=\u001b[39mbbox_extra_artists)\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1786\u001b[0m         bbox \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mget_tightbbox(renderer)\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    449\u001b[0m     warn_deprecated(\n\u001b[0;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:4415\u001b[0m, in \u001b[0;36m_AxesBase.get_tightbbox\u001b[1;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[0;32m   4413\u001b[0m bbox_artists \u001b[38;5;241m=\u001b[39m bbox_extra_artists\n\u001b[0;32m   4414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_artists \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4415\u001b[0m     bbox_artists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_bbox_extra_artists()\n\u001b[0;32m   4417\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m bbox_artists:\n\u001b[0;32m   4418\u001b[0m     bbox \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mget_tightbbox(renderer)\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:4335\u001b[0m, in \u001b[0;36m_AxesBase.get_default_bbox_extra_artists\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4330\u001b[0m \u001b[38;5;66;03m# always include types that do not internally implement clipping\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;66;03m# to Axes. may have clip_on set to True and clip_box equivalent\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \u001b[38;5;66;03m# to ax.bbox but then ignore these properties during draws.\u001b[39;00m\n\u001b[0;32m   4333\u001b[0m noclip \u001b[38;5;241m=\u001b[39m (_AxesBase, maxis\u001b[38;5;241m.\u001b[39mAxis,\n\u001b[0;32m   4334\u001b[0m           offsetbox\u001b[38;5;241m.\u001b[39mAnnotationBbox, offsetbox\u001b[38;5;241m.\u001b[39mOffsetBox)\n\u001b[1;32m-> 4335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mget_visible() \u001b[38;5;129;01mand\u001b[39;00m a\u001b[38;5;241m.\u001b[39mget_in_layout()\n\u001b[0;32m   4336\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(a, noclip) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m a\u001b[38;5;241m.\u001b[39m_fully_clipped_to_axes())]\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:4336\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   4330\u001b[0m \u001b[38;5;66;03m# always include types that do not internally implement clipping\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;66;03m# to Axes. may have clip_on set to True and clip_box equivalent\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \u001b[38;5;66;03m# to ax.bbox but then ignore these properties during draws.\u001b[39;00m\n\u001b[0;32m   4333\u001b[0m noclip \u001b[38;5;241m=\u001b[39m (_AxesBase, maxis\u001b[38;5;241m.\u001b[39mAxis,\n\u001b[0;32m   4334\u001b[0m           offsetbox\u001b[38;5;241m.\u001b[39mAnnotationBbox, offsetbox\u001b[38;5;241m.\u001b[39mOffsetBox)\n\u001b[0;32m   4335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mget_visible() \u001b[38;5;129;01mand\u001b[39;00m a\u001b[38;5;241m.\u001b[39mget_in_layout()\n\u001b[1;32m-> 4336\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(a, noclip) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m a\u001b[38;5;241m.\u001b[39m_fully_clipped_to_axes())]\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\artist.py:879\u001b[0m, in \u001b[0;36mArtist._fully_clipped_to_axes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    873\u001b[0m clip_box \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_box()\n\u001b[0;32m    874\u001b[0m clip_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_path()\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_on()\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (clip_box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m clip_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (clip_box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 879\u001b[0m              \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(clip_box\u001b[38;5;241m.\u001b[39mextents \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mbbox\u001b[38;5;241m.\u001b[39mextents))\n\u001b[0;32m    880\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (clip_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    881\u001b[0m              \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(clip_path, TransformedPatchPath)\n\u001b[0;32m    882\u001b[0m              \u001b[38;5;129;01mand\u001b[39;00m clip_path\u001b[38;5;241m.\u001b[39m_patch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mpatch))\n",
      "File \u001b[1;32mc:\\Users\\alois\\anaconda3\\Lib\\site-packages\\matplotlib\\transforms.py:377\u001b[0m, in \u001b[0;36mBboxBase.extents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextents\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return (:attr:`x0`, :attr:`y0`, :attr:`x1`, :attr:`y1`).\"\"\"\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_points()\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict_and_visualize(model, test_dataset, device, n_images=len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualizes(models, dataset, device, n_images):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_images):\n",
    "            input_img, true_mask = dataset[i]\n",
    "            input_img_unsqueeze = input_img.unsqueeze(0).to(device)\n",
    "            sum_pred_masks = 0\n",
    "\n",
    "            # Ensemble Predict\n",
    "            for model in models:\n",
    "                pred_mask_logits = model(input_img_unsqueeze)\n",
    "                pred_mask_prob = torch.sigmoid(pred_mask_logits)\n",
    "                pred_mask = (pred_mask_prob > 0.5).float()\n",
    "                sum_pred_masks += pred_mask\n",
    "\n",
    "            # Majority voting or thresholding\n",
    "            final_pred_mask = (sum_pred_masks > len(models) // 2).float() \n",
    "\n",
    "            # Calculate Dice Coefficient\n",
    "            pred = final_pred_mask.squeeze()\n",
    "            dice_score = dice_coefficient(pred, true_mask.to(device), smooth=1e-6)\n",
    "            dice_scores.append(dice_score.item())\n",
    "\n",
    "            # Convert for visualization\n",
    "            final_pred_mask_np = final_pred_mask.squeeze().cpu().numpy()\n",
    "            input_img_np = input_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            input_img_np = (input_img_np * 255).astype(np.uint8)\n",
    "            true_mask_np = true_mask.squeeze().cpu().numpy() if true_mask is not None else None\n",
    "\n",
    "            visualize(\n",
    "                image=input_img_np,\n",
    "                true_mask=true_mask_np,\n",
    "                predicted_mask=final_pred_mask_np\n",
    "            )\n",
    "\n",
    "    # Print the average Dice score at the end\n",
    "    average_dice_score = np.mean(dice_scores)\n",
    "    print(f'Average Dice Coefficient over {n_images} images: {average_dice_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "\n",
    "def evaluate_models(models, dataset, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    dice_scores = []\n",
    "    f1_scores = []\n",
    "    ious = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_img, true_mask in dataset:\n",
    "            input_img = input_img.unsqueeze(0).to(device)\n",
    "            true_mask = true_mask.unsqueeze(0).to(device)\n",
    "            avg_pred_prob = 0\n",
    "\n",
    "            for model in models:\n",
    "                output = model(input_img)\n",
    "                pred_mask_prob = torch.sigmoid(output)\n",
    "                avg_pred_prob += pred_mask_prob / len(models)\n",
    "\n",
    "            # Thresholding the averaged probability map\n",
    "            final_pred_mask = (avg_pred_prob > 0.5).float()\n",
    "\n",
    "            # Flatten the tensors for compatibility with sklearn metrics\n",
    "            y_true = true_mask.view(-1).cpu().numpy()\n",
    "            y_pred = final_pred_mask.view(-1).cpu().numpy()\n",
    "\n",
    "            # Compute metrics\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "            dice_score = dice_coefficient(final_pred_mask, true_mask, smooth=1e-6).item()\n",
    "            iou = jaccard_score(y_true, y_pred, zero_division=1)\n",
    "\n",
    "            # Append metrics\n",
    "            f1_scores.append(f1)\n",
    "            dice_scores.append(dice_score)\n",
    "            ious.append(iou)\n",
    "\n",
    "    # Compute mean of all metrics\n",
    "    metrics = {\n",
    "        'Mean F1 Score': np.mean(f1_scores),\n",
    "        'Mean Dice Coefficient': np.mean(dice_scores),\n",
    "        'Mean IoU': np.mean(ious)\n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alois\\AppData\\Roaming\\Python\\Python311\\site-packages\\albumentations\\augmentations\\blur\\transforms.py:189: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from R2Attention_unet import R2AttU_Net\n",
    "from resnet152_attention_unet import RAUNet\n",
    "from resnet152_unet import UNetWithResnet152Encoder\n",
    "\n",
    "name_model1 = 'tuned_resnet152_attention_unet_power_dice_dice'\n",
    "name_model2 = 'resnet152_SEB_V5'\n",
    "name_model3 = 'best_model_DRIVE_tuned152'\n",
    "name_model5 = 'tuned_resnet152_attention_unet_power_dice'\n",
    "\n",
    "test_dataset = CustomDataset('../data_model_V5/test/images', '../data_model_V5/test/masks', augment=False, denoise=False)\n",
    "\n",
    "#model = UNetWithResnet152Encoder(n_classes=1).to(device)\n",
    "#model = R2AttU_Net(img_ch=3, output_ch=1).to(device)\n",
    "model1 = RAUNet(num_classes=1, num_channels=3, pretrained=True).to(device)\n",
    "model1.load_state_dict(torch.load(f'output_NEW/{name_model1}.pth'))\n",
    "model1.eval()\n",
    "\n",
    "model2 = UNetWithResnet152Encoder_SEB(n_classes=1).to(device)\n",
    "model2.load_state_dict(torch.load(f'output_NEW/{name_model2}.pth'))\n",
    "model2.eval()\n",
    "\n",
    "model3 = UNetWithResnet152Encoder(n_classes=1).to(device)\n",
    "model3.load_state_dict(torch.load(f'output_dir/{name_model3}.pth'))\n",
    "model3.eval()\n",
    "\n",
    "                            \n",
    "                                            \n",
    "model5 = RAUNet(num_classes=1, num_channels=3, pretrained=True).to(device)\n",
    "model5.load_state_dict(torch.load(f'output_NEW/{name_model5}.pth'))\n",
    "model5.eval()\n",
    "\n",
    "model_list = [model1, model2, model3, model5]\n",
    "metrics = evaluate_models(model_list, test_dataset, device)\n",
    "\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f'{metric_name}: {metric_value:.4f}')\n",
    "\n",
    "#predict_and_visualizes(model_list, test_dataset, device, n_images=len(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
